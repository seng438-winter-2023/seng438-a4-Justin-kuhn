**SENG 438 - Software Testing, Reliability, and Quality**

**Lab. Report \#4 – Mutation Testing and Web app testing**

| Group \#:      |     |
| -------------- | --- |
| Student Names: | Aayush Dahal    |
|                | Justin Kuhn    |
|                | Sheroze Nasir    |

# Introduction

Mutation testing is a widely used software testing technique that is used to improve upon existing test suites by measuring the quality of the test cases written. It is different from White Box testing, in that the actual source code is modified in various ways, and then tested against a test suite. If the external behavior of the code changes but a test still passes, this is known as a mutation survival, and must be looked into. If the external behavior changes but the test fails, this is desired behavior and means the mutation was killed. In this lab, we will use PIT as our mutation testing tool.

Another objective of this lab is to test website GUIs, using an automated web testing tool. In this case, we are using Selenium IDE, and testing the interface of Amazon.ca. We will have to add verification criteria to our test cases, in order to determine whether our tests pass or fail against the website. We will design test cases using the record feature, which records all of a user’s inputs and creates a macro for playback to be used during the test.

By the end of this lab, we will have gained experience using a mutation testing tool, and learned about the benefits of mutation testing that we can apply to other software projects. We will be able to identify equivalent mutants, as well as surviving mutants and be able to write test cases to kill such mutants based off a mutation score. Additionally, we will have learned the basics of a web GUI testing tool, and be able to record macros and use them for testing various features of websites.

# Analysis of 10 Mutants of the Range class 

![10RangeMutants](https://user-images.githubusercontent.com/100978731/226087314-e5aa37a5-37d2-4fc6-ac79-c4d93b6ce3b6.PNG)

The above mutants are for the method public double constrain(double value) from the Range class. 

The line “double result = value” becomes “double result = -value”. Our test suite had tests to account for this, and as a result the mutant was killed.

The line “double result = value” becomes “double result = value++”. Because this uses a postfix operator, our unit tests were not able to tell the difference between the values, and then mutant survived.

The line “double result = value” becomes “double result = value--”. Because this uses a postfix operator, our unit tests were not able to tell the difference between the values, and then mutant survived.

The line “double result = value” becomes “double result = ++value”. Because this uses a prefix operator, our unit tests were able to tell the difference between the values, and the mutant was killed.

The line “double result = value” becomes “double result = --value”. Because this uses a prefix operator, our unit tests were able to tell the difference between the values, and the mutant was killed.

The line “if (!contains(value))” becomes “if (contains(value))”. Our test suite had tests to account for this, and as a result the mutant was killed.

The line “if (!contains(value))” has the call to the contains() method removed from it. Our test suite could not tell the difference between whether contains() was called or not, and as a result the mutant survived.

The line “if (!contains(value))” becomes “if(false)”. This condition will never be true, and as such our test suite had tests to account for this. As a result the mutant was killed.

The line “if (!contains(value))” becomes “if(true)”. This condition will always be true, which reveals that our code does not have tests that depend on the condition being false. As a result, the mutant survived.

The line “if (!contains(value))” becomes “if(!contains(-value))”. This is a severely unexpected condition that we never thought we would reach when designing our test suite. Thus, we didn’t have tests to cover this case, and as a result the mutant survived.

# Report all the statistics and the mutation score for each test class

Range Statistics:
![RangeStats](https://user-images.githubusercontent.com/100978731/226087367-b57bc547-0c84-4e4a-b259-dc373bae1c41.PNG)

Range Mutation Score:
![RangeMutation](https://user-images.githubusercontent.com/100978731/226087371-5a631f13-5d05-4de7-998f-f41d868730b9.PNG)

DataUtilities Statistics:
![DataUtilitiesStats](https://user-images.githubusercontent.com/100978731/226087428-271fdc3f-c238-454e-9c8f-aa24e7e9283b.png)

DataUtilities Mutation Score:
![DataUtilitiesMutation](https://user-images.githubusercontent.com/100978731/226087435-7c5e7dc1-ea42-4374-b634-e647d5ca9fc0.PNG)

# Analysis drawn on the effectiveness of each of the test classes

The mutation score for the original RangeTest file was 68%. This shows that the original test suite did a decent job of covering possible mutations, however there were still a good amount of cases that went unchecked. In our findings, we discovered that most of these cases were situations where variables would take extremely unpredictable values, such as negations of themselves and being swapped with another variable. This supports the idea that our method and branch coverages were solid, but our condition coverage could use some work.

The mutation score for the original DataUtilitiesTest file was 77%. This shows that the original test suite did a fairly good job of covering possible mutations, and there were only some cases that went unchecked. Similar to Range, we found that these were cases where variables would take unexpected values that we did not think of during the previous assignments. This demonstrates how our method and branch coverages were high, while our condition coverage was not exhausted to its full extent.

# A discussion on the effect of equivalent mutants on mutation score accuracy

Equivalent mutants are very problematic when it comes to mutation testing. These are mutations that appear to change the internal source code, but do not change the external behavior of the system. As such, they are unable to be killed during mutation testing due to the fact that their behavior is indistinguishable from the original functionality. As a result, PIT will report it as a survived mutant, when in reality this is a false negative. This will make the mutation score lower than it should be.

An example of equivalent mutants we found during testing were cases where entire conditions were negated, as well as comparison operators. For instance, in the method equals() in Range, there is the line “if(!(this.lower == range.lower))”. During mutation testing, this line would be changed to “if(this.lower != range.lower)”. While the lines are different, they are logically the same and thus the difference would not be detected by our unit tests.

Similarly, DataUtilities had a fair share of equivalent mutants. One of them was changing “for (int i = 0; i < a.length; i++)” to “for (int i = 0; i != a.length; i++)”, and another was changing “if(!Arrays.equals(a[i], b[i]))” to “if (a[i] > b[i])”. These are both in the equal() function of DataUtilities, and in both cases the condition is effectively the same, as the for loop starts and stops at the same point regardless of the comparison being used. 

To find equivalent mutants, we first isolated the mutants that survived, and then checked the individual cases to see if they changed the external behavior of the system. If they didn’t, they would be considered an equivalent mutant, and if they did, they would be considered a surviving mutant that we would then try to kill.

# A discussion of what could have been done to improve the mutation score of the test suites

Through our observations, we noticed that the Range class was not tested as thoroughly as it could have been. When designing the test cases in the previous assignments, we did not implement boundary value analysis as good as we initially thought. To improve the mutation score, we could have written additional tests to cover values less than, equal to and greater than the lower and upper bounds for each method in the Range class.

Furthermore, we noticed that the DataUtilities class was tested considerably well. This is because almost all of the mutants that survived were either due to equivalent mutants, or unreachable code. Therefore, not much could be done to improve the DataUtilitiesTest file, besides adding additional test cases to cover the getCumulativePercentages() method, which was a method that was not tested as thoroughly as it could have been.

# Why do we need mutation testing? Advantages and disadvantages of mutation testing

Throughout this course, we have learned various methods of testing software. Starting with Black Box testing, we test the system against the requirements that are given to us in the official documentation. Next we tried White Box testing, which is testing the system against its own source code. Now we have mutation testing, which is running tests to assess the quality of the tests we have written. We need mutation testing to ensure that our test cases are actually testing what they’re supposed to test, and not passing due to coincidence or cutting corners. An advantage of mutation testing is by combining it with coverage testing, you can create a test suite that not only covers the source code, but also tests the source code correctly. Some disadvantages of mutation testing are that the process itself can be quite slow, a lot of unrealistic scenarios can arise that would never be encountered normally, and equivalent mutations frequently appear, making it nearly impossible to reach a 100% mutation score.

# Explain your SELENUIM test case design process

We chose Amazon as our testing web application for the Selenium WebDriver plug-in. Our initial step in the design process consisted of identifying the web application’s scope. We did this by exploring the web application to create a list of all its features and functions. Afterwards, we provided two types of cases when testing each function of the web application, “success” cases and “fail” cases. For some of the features a “fail” case was not possible. With this information and plan of approach, we began the testing phase. We recorded our tests and used the automated verification wherever possible, usually in “fail” cases to identify the test had failed. Once our testing was complete we had a rerun of them to make sure they worked without issue. 

Some of our test cases relied on being “logged in” or “logged out” whereas others were independent of this feature. The “login” and “register” test cases only work if the user has not already logged in, otherwise there would be some obvious faults. However, the changing address function can only work if the user is logged in. Finally, the language changing success function only works if the language is initially in english as it changes it to french.

# Explain the use of assertions and checkpoints

Automated verification points are vital features in the Selenium IDE. The fundamental of testing is comparing the expected value with actual value to determine whether the test case has “passed” or “failed”. The use of these verification points helps the system verify the status of a test case. In our specific tests, the verification checkpoints were mostly used to identify whether the test case had “failed” due to inputting faulty/inaccurate inputs. The output would be some sort of error message to indicate the failure of the test case, so we used the text to verify whether the test had failed. 
List of verification points used in test cases:
LoginFail test case had a verification point with the value “We cannot find an account with that e-mail address”. 
ChangeAddressFail test case had a verification point with the value “Please enter a valid Canada postal code”
RegisterFail test case had a verification point with the value “Wrong or invalid e-mail address or mobile phone number. Please correct it and try again.”
RegisterSuccess test case had a verification point with the value “Verify email address”
SearchFail test case had a verification point with the value “No results for”

Automated verification points are essentially forms of validation and are used to detect whether an element is present or not to determine whether the test “passed” or “failed”. 

# how did you test each functionaity with different test data

For each functionality we came up with a test case that “passed” and one that “failed”. Some functionality test cases are not possible to “fail”. These include the test cases PriceRangeSuccess and SortingSuccess. 

For Price range it was not possible to have the system “fail” because even if the input was a string of letters, the web application would simply ignore it. There was no error/fail message. The inputs can also range between really big numbers and really small numbers. Hence, there was no way to make this function fail. 

For Sorting the system only had a selection of sorting modes. This means no input can be given that is not already recognized. Hence, there is no way to make this function fail.

The rest of the tests had two different test cases, “passed” and “failed”. The way we tested whether the test case failed is by inputting; wrong email, random text, incorrect information and to detect this, we used the verification point feature to detect the output.

# Discuss advantages and disadvantages of Selenium vs. Sikulix

Selenium is a WebDriver automation tool that can test web applications. Sikulix is more of an image based automation tool that can test not just web applications but also desktop applications or different types of GUIs.

![Adv](https://user-images.githubusercontent.com/100978731/226087657-c355207f-477c-4c9d-aea7-6ad825acb73c.PNG)

# How the team work/effort was divided and managed

For this lab, our group met in person to familiarize ourselves with the required software, and begin dividing the assigned work. We eventually decided that it was best for each of us to gain experience with each component of the lab, so we would work together on each part in order. First, we ran mutation tests on our original test suite. Then, we collectively brainstormed ideas for how we could improve the mutation coverage. After implementing the new tests, we helped each other get acquainted with the Selenium IDE, and each member thought of 2 test cases that can be performed on Amazon’s web GUI.

# Difficulties encountered, challenges overcome, and lessons learned

Through this lab, we had two major difficulties. The first one was in the source code of the DataUtilities class, where many lines were unreachable due to syntax errors by the developers. This made it frustrating to write tests to cover these, as the mutations kept surviving and we weren’t immediately sure why. The second challenge was using Selenium IDE. Our group and others noticed that it gets incredibly laggy while recording for extended periods of time, and sometimes crashes in the middle of recording. This made it incredibly frustrating to use, as we would have to frequently re-record tests and restart the program. A lesson we learned from this lab is to perhaps use a different tool for testing web interfaces in the future.

# Comments/feedback on the lab itself

Overall, our group thought this lab was very interesting and helpful for introducing us to mutation testing. One thing however, is that the instructions could make it more clear that running PIT takes a very long time. Many students including ourselves were confused when we saw the console fill up with errors, only to give us the results of the mutation test 25 minutes later. If there was some indication that these errors were normal, it would have saved some time and confusion. Besides that, we feel that this lab was a positive learning experience.
